{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments Cleaning\n",
    "\n",
    "### Contents\n",
    "\n",
    "1. Python Libraries\n",
    "2. Function containing Text Preprocessing Techiniques done\n",
    "> * Case Folding\n",
    "> * Removal of Non-English words\n",
    "> * Removal of Punctuations\n",
    "> * Removal of Stopwords\n",
    "> * Removal of Emojis\n",
    "> * Word Stemming\n",
    "3. Words Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "import seaborn as sns\n",
    "import demoji\n",
    "\n",
    "from collections import Counter\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function containing Text Preprocessing Techiniques done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fxn_case_folding(var_input):\n",
    "    \"\"\"\n",
    "    Preprocessing: Case Folding\n",
    "    \"\"\"\n",
    "    return var_input.lower()\n",
    "\n",
    "# To remove vernacular words, acronyms and wrong spellings.\n",
    "def fxn_remove_non_english(input_text):\n",
    "    \"\"\"\n",
    "    Preprocessing: Removing non-english words\n",
    "    \"\"\"\n",
    "    remove_words = \" \".join([w for w in input_text.split() if w in words.words()])\n",
    "    return remove_words\n",
    "\n",
    "def fxn_punctuation(var_input_text):\n",
    "    \"\"\"\n",
    "    Preprocessing: Punctuation Removal\n",
    "    \"\"\"\n",
    "    var_output_text = re.sub(\"[%s]\" % re.escape(string.punctuation), \" \", var_input_text)\n",
    "    var_output_text = re.sub(\"[%s]\" % re.escape(string.punctuation), \" \", var_output_text)\n",
    "    var_output_text = re.sub('\\w*\\d\\w*', '', var_output_text) # HINT: lookup isalpha() function\n",
    "    return var_output_text\n",
    "\n",
    "def fxn_stopwords(var_input_text):\n",
    "    \"\"\"\n",
    "    Preprocessing: Stopwords Removal\n",
    "    \"\"\"\n",
    "    var_etd_stop = \" \".join([\n",
    "        var_etd_word for var_etd_word in var_input_text.split() \n",
    "        if var_etd_word not in stopwords.words('english')\n",
    "    ])\n",
    "    return var_etd_stop\n",
    "\n",
    "def fxn_demoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return(emoji_pattern.sub(r'', text))\n",
    "\n",
    "def fxn_stem(var_input_text):\n",
    "    \"\"\"\n",
    "    Preprocessing: Stemming\n",
    "    \"\"\"\n",
    "    var_stemmer = PorterStemmer()\n",
    "    var_output_text = \" \".join([\n",
    "        var_stemmer.stem(var_etd_word) for var_etd_word in var_input_text.split() \n",
    "    ])\n",
    "    return var_output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat -n mwebantu_scraped_comments.txt| head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate the csv file for cleaning\n",
    "!cat mwebantu_scraped_comments.txt > mwebantu_scraped_comments_cleaned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l mwebantu_scraped_comments_cleaned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_post_comments = pd.read_csv('mwebantu_scraped_comments_cleaned.txt', sep = '|', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_post_comments.columns = ['UserComments']\n",
    "facebook_post_comments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_post_comments['UserComments'].replace('', np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there any Null entries in the Series\n",
    "facebook_post_comments['UserComments'].isna().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the current length of the pandas Series\n",
    "len(facebook_post_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing every character to lowercase\n",
    "facebook_post_comments['UserComments'] = facebook_post_comments['UserComments'].apply(fxn_case_folding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_post_comments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords from the comments\n",
    "facebook_post_comments['UserComments'] = facebook_post_comments['UserComments'].apply(fxn_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_post_comments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all the emojis from the comments\n",
    "facebook_post_comments['UserComments'] = facebook_post_comments['UserComments'].apply(fxn_demoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_post_comments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing every punction from the comments\n",
    "facebook_post_comments['UserComments'] = facebook_post_comments['UserComments'].apply(fxn_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_post_comments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing non-english words\n",
    "facebook_post_comments['UserComments'] = facebook_post_comments['UserComments'].apply(fxn_remove_non_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_post_comments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(facebook_post_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_post_comments.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there any Null entries in the Series\n",
    "facebook_post_comments['UserComments'].isna().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing empty entries with NAN\n",
    "facebook_post_comments['UserComments'].replace('', np.nan, inplace = True)\n",
    "facebook_post_comments['UserComments'].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all rows that have NAN\n",
    "facebook_post_comments.dropna(subset = ['UserComments'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(facebook_post_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_post_comments['UserComments'].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming all the comments\n",
    "facebook_post_comments['UserComments'] = facebook_post_comments['UserComments'].apply(fxn_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_post_comments.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(facebook_post_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_post_comments['UserComments'].replace('', np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_post_comments['UserComments'].isna().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a word cloud to see most typed words which will\n",
    "# this will be used to identify key words when classifying the comments\n",
    "most_typed_words = WordCloud(stopwords = stopwords.words('english'), background_color = 'black', colormap = 'Dark2', max_font_size = 100, random_state = 42)\n",
    "most_typed_words.generate(' '.join(facebook_post_comments['UserComments']))\n",
    "\n",
    "plt.figure(figsize = (15,10))\n",
    "plt.imshow(most_typed_words)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(facebook_post_comments, 'facebook_post_comments_series.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit3450f985afdc4897a0c9ef51fad1462c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
