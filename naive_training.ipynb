{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing and Model Training\n",
    "### Contents\n",
    "1. Python Libraries\n",
    "2. Function containing Text Preprocessing Techiniques done\n",
    "> * Case Folding\n",
    "> * Removal of Punctuations\n",
    "> * Removal of Stopwords\n",
    "> * Removal of Emojis\n",
    "> * Word Stemming\n",
    "3. Model Training\n",
    "> * Holdout Method\n",
    "> * Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "import seaborn as sns\n",
    "import demoji\n",
    "import joblib\n",
    "\n",
    "from collections import Counter\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function containing Text Preprocessing Techiniques done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fxn_case_folding(var_input):\n",
    "    \"\"\"\n",
    "    Preprocessing: Case Folding\n",
    "    \"\"\"\n",
    "    return var_input.lower()\n",
    "\n",
    "def fxn_remove_non_english(input_text):\n",
    "    \"\"\"\n",
    "    Preprocessing: Removing non-english words\n",
    "    \"\"\"\n",
    "    remove_words = \" \".join([w for w in input_text.split() if w in words.words()])\n",
    "    return remove_words\n",
    "\n",
    "def fxn_punctuation(var_input_text):\n",
    "    \"\"\"\n",
    "    Preprocessing: Punctuation Removal\n",
    "    \"\"\"\n",
    "    var_output_text = re.sub(\"[%s]\" % re.escape(string.punctuation), \" \", var_input_text)\n",
    "    var_output_text = re.sub(\"[%s]\" % re.escape(string.punctuation), \" \", var_output_text)\n",
    "    var_output_text = re.sub('\\w*\\d\\w*', '', var_output_text) # HINT: lookup isalpha() function\n",
    "    return var_output_text\n",
    "\n",
    "def fxn_stopwords(var_input_text):\n",
    "    \"\"\"\n",
    "    Preprocessing: Stopwords Removal\n",
    "    \"\"\"\n",
    "    var_etd_stop = \" \".join([\n",
    "        var_etd_word for var_etd_word in var_input_text.split() \n",
    "        if var_etd_word not in stopwords.words('english')\n",
    "    ])\n",
    "    return var_etd_stop\n",
    "\n",
    "def fxn_demoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return(emoji_pattern.sub(r'', text))\n",
    "\n",
    "def fxn_stem(var_input_text):\n",
    "    \"\"\"\n",
    "    Preprocessing: Stemming\n",
    "    \"\"\"\n",
    "    var_stemmer = LancasterStemmer()\n",
    "    var_output_text = \" \".join([\n",
    "        var_stemmer.stem(var_etd_word) for var_etd_word in var_input_text.split() \n",
    "    ])\n",
    "    return var_output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset = pd.read_csv('train.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset = pd_dataset.iloc[:,2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array = pd_dataset['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset = pd_dataset.drop(['toxic'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset['comment_text'] = pd_dataset['comment_text'].apply(fxn_case_folding)\n",
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset['comment_text'] = pd_dataset['comment_text'].apply(fxn_punctuation)\n",
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset['comment_text'] = pd_dataset['comment_text'].apply(fxn_stopwords)\n",
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset['comment_text'] = pd_dataset['comment_text'].apply(fxn_demoji)\n",
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd_dataset['comment_text'] = pd_dataset['comment_text'].apply(fxn_stem)\n",
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformation_tfidv = TfidfVectorizer(max_features=534, min_df=5, max_df=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the data to tfidf\n",
    "train_data_transformation_tfidv.fit_transform(pd_dataset['comment_text'])\n",
    "#variable to hold transformed data\n",
    "pd_dataset_transformed = train_data_transformation_tfidv.fit_transform(pd_dataset['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformation_tfidv.get_feature_names()\n",
    "#len(train_data_transformation_tfidv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data_transformation_tfidv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset_feature_names = pd.DataFrame(pd_dataset_transformed.toarray(), columns = train_data_transformation_tfidv.get_feature_names())\n",
    "\n",
    "pd_dataset_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pd_dataset_feature_names, 'training_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the feature as input data for training\n",
    "X = pd_dataset_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the Toxic labeled data as output for training and also converting to them to numpy array\n",
    "y = y_array.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using holdout method to split dataset into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('text_classifier', 'wb') as picklefile:\n",
    "    pickle.dump(classifier,picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_classifier', 'rb') as training_model:\n",
    "    model = pickle.load(training_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print(classification_report(y_test, y_pred2))\n",
    "print(accuracy_score(y_test, y_pred2)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit3450f985afdc4897a0c9ef51fad1462c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
